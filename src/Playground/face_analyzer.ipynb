{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qihan/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from facetorch import FaceAnalyzer\n",
    "from omegaconf import OmegaConf\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from typing import Dict\n",
    "import operator\n",
    "import torchvision\n",
    "import os\n",
    "import operator\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle5 as pickle\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize\n",
    "path_config=\"gpu.config.yml\"\n",
    "cfg = OmegaConf.load(path_config)\n",
    "analyzer = FaceAnalyzer(cfg.analyzer)\n",
    "\n",
    "#Warm up\n",
    "path_img_input=\"./test_error_1/frame_0.png\"\n",
    "response=None\n",
    "# warmup\n",
    "response = analyzer.run(\n",
    "        path_image=path_img_input,\n",
    "        batch_size=cfg.batch_size,\n",
    "        fix_img_size=cfg.fix_img_size,\n",
    "        return_img_data=False,\n",
    "        include_tensors=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_string(input_dir):\n",
    "    # Split the input directory into parts\n",
    "    parts = input_dir.split('/')\n",
    "    \n",
    "    # Get the last part of the directory\n",
    "    last_part = parts[-1]\n",
    "    \n",
    "    # Split the last part by underscore\n",
    "    last_part_parts = last_part.split('_')\n",
    "    \n",
    "    # Get the video name and frame rate\n",
    "    video_name = \"_\".join(last_part_parts[:3])\n",
    "    frame_rate = last_part_parts[6]\n",
    "    \n",
    "    # Generate the string\n",
    "    result = f\"Similarity Matrix, {video_name}, fps={frame_rate}\"\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_dict(input_dir):\n",
    "    # Get a list of all image files in the \"./test\" directory\n",
    "    image_files = [file for file in os.listdir(input_dir) if file.endswith(\".png\")]\n",
    "\n",
    "    # Create an empty dictionary to store the responses\n",
    "    responses_dict = {}\n",
    "    responses_dict_path = os.path.join(input_dir, 'responses_dict.pkl')\n",
    "    if os.path.exists(responses_dict_path):\n",
    "        # Load the responses_dict from the pickle file\n",
    "        saved_responses_dict = read_responses_dict(input_dir)\n",
    "    \n",
    "    # Remove the image files that are already in the responses_dict keys\n",
    "    image_files = [file for file in image_files if ((file not in saved_responses_dict.keys()) and (file.endswith(\".png\")))]\n",
    "    # Iterate over each image file with a progress bar\n",
    "    for image_file in tqdm(image_files):\n",
    "        \n",
    "        # Get the full path of the image file\n",
    "        image_path = os.path.join(input_dir, image_file)\n",
    "        \n",
    "        # Run the analyzer on the image\n",
    "        response = analyzer.run(\n",
    "            path_image=image_path,\n",
    "            batch_size=cfg.batch_size,\n",
    "            fix_img_size=cfg.fix_img_size,\n",
    "            return_img_data=cfg.return_img_data,\n",
    "            include_tensors=cfg.include_tensors,\n",
    "        )\n",
    "        if len(response.faces) == 0:\n",
    "            print(f\"No face detected in {image_file}\")\n",
    "            continue\n",
    "        # Add the response to the dictionary with the image file name as the key\n",
    "        responses_dict[image_file] = response.faces[0].preds[\"embed\"].logits\n",
    "    \n",
    "    responses_dict = dict(sorted(responses_dict.items(), key=lambda x: int(''.join(filter(str.isdigit, x[0])))))\n",
    "    return responses_dict\n",
    "\n",
    "def read_responses_dict(input_dir):\n",
    "    # Check if the responses_dict file exists\n",
    "    responses_dict_path = os.path.join(input_dir, 'responses_dict.pkl')\n",
    "    if not os.path.exists(responses_dict_path):\n",
    "        print(\"responses_dict.pkl file does not exist in the input directory.\")\n",
    "        return None\n",
    "        \n",
    "    # Load the responses_dict from the pickle file\n",
    "    with open(responses_dict_path, 'rb') as file:\n",
    "        responses_dict = pickle.load(file)\n",
    "    return responses_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_similarity_matrix(input_dir, compare_dir=None):\n",
    "    if os.path.exists(os.path.join(input_dir, 'similarity_matrix_compare_neutral.png')):\n",
    "        return\n",
    "    \n",
    "    responses_dict_1 = read_responses_dict(input_dir)\n",
    "    if compare_dir is not None:\n",
    "        responses_dict_2 = read_responses_dict(compare_dir)\n",
    "    else:\n",
    "        responses_dict_2 = responses_dict_1\n",
    "    similarity_matrix = {}\n",
    "\n",
    "    # Iterate over each pair of keys in responses_dict\n",
    "    for key_1, value_1 in responses_dict_1.items():\n",
    "        for key_2, value_2 in responses_dict_2.items():\n",
    "            # Calculate the cosine similarity between value_1 and value_2\n",
    "            similarity = cosine_similarity(value_1, value_2, dim=0).item()\n",
    "            \n",
    "            # Add the similarity to the similarity matrix with the key pair as the key\n",
    "            similarity_matrix[key_1 + \"+\" + key_2] = similarity\n",
    "\n",
    "    # Convert the similarity matrix to a dataframe\n",
    "    similarity_df = pd.DataFrame.from_dict(similarity_matrix, orient='index')\n",
    "    # Reset the index and split the key pair into separate columns\n",
    "    similarity_df.reset_index(inplace=True)\n",
    "    similarity_df[['key_1', 'key_2']] = similarity_df['index'].str.split('+', expand=True)\n",
    "    # Convert the key_1 and key_2 columns to numeric type\n",
    "    similarity_df['key_1'] = similarity_df['key_1'].str.extract('(\\d+)', expand=False).astype(int)\n",
    "\n",
    "    similarity_df['key_2'] = similarity_df['key_2'].str.extract('(\\d+)', expand=False).astype(int)\n",
    "\n",
    "    # Sort the dataframe by key_1 and key_2 columns\n",
    "    similarity_df = similarity_df.sort_values(['key_1', 'key_2'])\n",
    "\n",
    "    # Pivot the dataframe to create a matrix-like structure\n",
    "    similarity_matrix_df = similarity_df.pivot(index='key_1', columns='key_2', values=0)\n",
    "\n",
    "    # Create a heatmap of the similarity matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(similarity_matrix_df, cmap='coolwarm', fmt=\".2f\")\n",
    "    if compare_dir is not None:\n",
    "        plt.title(generate_string(input_dir))\n",
    "    else:\n",
    "        plt.title(generate_string(input_dir)+\"compared to neutral baseline\")\n",
    "    plt.xlabel('frames')\n",
    "    plt.ylabel('frames')\n",
    "    if compare_dir is not None:\n",
    "        plt.savefig(os.path.join(input_dir, 'similarity_matrix_compare_neutral.png'))\n",
    "    else:\n",
    "        plt.savefig(os.path.join(input_dir, 'similarity_matrix.png'))\n",
    "    return\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_talking_frames(input_dir, last_time_above_threshold):\n",
    "    # Load the similarity matrix\n",
    "    with open(os.path.join(input_dir, 'responses_dict.pkl'), 'rb') as file:\n",
    "        responses_dict = pickle.load(file)\n",
    "    \n",
    "    # Get the frame rate of the video\n",
    "    frame_rate = 30\n",
    "    key_to_delete=[]\n",
    "    for key in responses_dict.keys():\n",
    "    # Get the time of each frame\n",
    "        times = int(key.split('_')[1].split('.')[0]) / frame_rate\n",
    "        if times < last_time_above_threshold:\n",
    "            key_to_delete.append(key)\n",
    "    for key in key_to_delete:\n",
    "        del responses_dict[key]\n",
    "    # Save the new dictionary as a pickle file\n",
    "    with open(os.path.join(input_dir, 'responses_dict.pkl'), 'wb') as file:\n",
    "        pickle.dump(responses_dict, file)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with one episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import wave\n",
    "path_to_video_dir = \"/media/qihan/CORSAIR/Test/Data_clean/User_3/video_2/3_0_3_video_2_framerate_3_faces\"\n",
    "path_to_audio_file = re.sub(r'_video_2_framerate_\\d+_faces', '_audio.wav', path_to_video_dir).replace('video_2', 'audio')\n",
    "\n",
    "# Specify the path to the audio file\n",
    "\n",
    "# Open the audio file\n",
    "def get_auio_length(path_to_audio_file):\n",
    "    with wave.open(path_to_audio_file, 'rb') as audio_file:\n",
    "        # Get the number of frames in the audio file\n",
    "        num_frames = audio_file.getnframes()\n",
    "        \n",
    "        # Get the frame rate of the audio file\n",
    "        frame_rate = audio_file.getframerate()\n",
    "        \n",
    "        # Calculate the length of the audio file in seconds\n",
    "        audio_length = num_frames / frame_rate\n",
    "    return audio_length\n",
    "\n",
    "last_speech_time = get_auio_length(path_to_audio_file)\n",
    "print(last_speech_time)\n",
    "remove_talking_frames(path_to_video_dir, last_speech_time)\n",
    "get_similarity_matrix(path_to_video_dir)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch process all episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "super_dir = \"/media/qihan/CORSAIR/Test/Data_clean/User_3/video_2\"\n",
    "\n",
    "# Get a list of all subfolders in super_dir\n",
    "subfolders = [folder for folder in os.listdir(super_dir) if os.path.isdir(os.path.join(super_dir, folder))]\n",
    "\n",
    "# Iterate over each subfolder with a progress bar\n",
    "for subfolder in tqdm(subfolders):\n",
    "    input_dir = os.path.join(super_dir, subfolder)\n",
    "    # Get the audio file path\n",
    "    path_to_audio_file = re.sub(r'_video_2_framerate_\\d+_faces', '_audio.wav', input_dir).replace('video_2', 'audio')\n",
    "    # Get the last time with speech\n",
    "    print(path_to_audio_file)\n",
    "    last_speech_time = get_auio_length(path_to_audio_file)\n",
    "    print(last_speech_time)\n",
    "    # Remove the frames with speech\n",
    "    get_response_dict(input_dir)\n",
    "    remove_talking_frames(input_dir, last_speech_time)\n",
    "    get_similarity_matrix(input_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "input_dir = \"/media/qihan/CORSAIR/Test/Data_clean/User_3/neutral_baseline\"\n",
    "big_dict = {}\n",
    "\n",
    "# Get a list of all subfolders in input_dir\n",
    "subfolders = [folder for folder in os.listdir(input_dir) if os.path.isdir(os.path.join(input_dir, folder))]\n",
    "\n",
    "# Iterate over each subfolder\n",
    "for subfolder in subfolders:\n",
    "    # Get the path to the responses_dict.pkl file\n",
    "    responses_dict_path = os.path.join(input_dir, subfolder, 'responses_dict.pkl')\n",
    "    \n",
    "    # Check if the responses_dict.pkl file exists\n",
    "    if os.path.exists(responses_dict_path):\n",
    "        # Load the responses_dict from the pickle file\n",
    "        with open(responses_dict_path, 'rb') as file:\n",
    "            responses_dict = pickle.load(file)\n",
    "        name=subfolder.split('_video')[0]\n",
    "        # Iterate over each key-value pair in responses_dict\n",
    "        for key, value in responses_dict.items():\n",
    "            # Create the new key in the format of \"{sub_folder_name}+{original key}\"\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Add the key-value pair to the big_dict\n",
    "            big_dict[str(len(big_dict)+1)] = value\n",
    "\n",
    "# Specify the path to save the pickle file\n",
    "pickle_file_path = os.path.join(input_dir, 'responses_dict.pkl')\n",
    "\n",
    "# Save the big_dict dictionary as a pickle file\n",
    "with open(pickle_file_path, 'wb') as file:\n",
    "    pickle.dump(big_dict, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with neutral baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:00<00:00, 1281.57it/s]\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"/media/qihan/CORSAIR/Test/Data_clean/User_3/video_2\"\n",
    "compare_dir = \"/media/qihan/CORSAIR/Test/Data_clean/User_3/neutral_baseline\"\n",
    "\n",
    "for subfolder in tqdm(os.listdir(input_dir)):\n",
    "    folder = os.path.join(input_dir, subfolder)\n",
    "    get_similarity_matrix(folder, compare_dir=compare_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index replacement completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path_to_csv = \"/media/qihan/CORSAIR/Test/Data_clean/User_3/catalog.csv\"\n",
    "# Read the CSV file\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# Read the CSV file\n",
    "with open(path_to_csv, mode='wr') as file:\n",
    "    csvFile = csv.reader(file)\n",
    "\n",
    "    task_cnt=0\n",
    "    for lines in csvFile:\n",
    "\n",
    "\n",
    "        json_string = lines[0]\n",
    "        data = json.loads(json_string)\n",
    "        error_type = data.get('error_type')\n",
    "        index=data.get('index')\n",
    "\n",
    "        if error_type == \"No Error\":\n",
    "            #read each entry from the pkl file and put them in a \n",
    "\n",
    "\n",
    "            \n",
    "        print(f\"Index: {index}, error: {error_type}\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
