{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert all videos into photos by 1 fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "directory = '/home/qihan/Playground/Data/User_3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_frames_from_video(video_file, output_folder, frame_rate=30):\n",
    "    #print(video_file)\n",
    "    video = cv2.VideoCapture(video_file)\n",
    "    num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    downsample_factor = int(video.get(cv2.CAP_PROP_FPS) / frame_rate)\n",
    "    folder_name = f'{output_folder}'\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    progress_bar = tqdm(total=num_frames, desc='Extracting frames', unit='frame')\n",
    "\n",
    "    for frame in range(0, num_frames, downsample_factor):\n",
    "        video.set(cv2.CAP_PROP_POS_FRAMES, frame)\n",
    "        ret, frame_data = video.read()\n",
    "        if ret:\n",
    "            image = Image.fromarray(cv2.cvtColor(frame_data, cv2.COLOR_BGR2RGB))\n",
    "            image.save(f'{folder_name}/frame_{frame}.png')\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    progress_bar.close()\n",
    "    video.release()\n",
    "\n",
    "def convert_videos_to_images(input_folder, output_folder, frame_rate=30):\n",
    "    video_files = [file for file in os.listdir(input_folder) if file.endswith('.mp4')]\n",
    "\n",
    "    for video_file in video_files:\n",
    "        video_name = os.path.splitext(video_file)[0]\n",
    "        subfolder_name = video_name.split('_')[1]\n",
    "        subfolder_path = os.path.join(input_folder, subfolder_name)\n",
    "        #print(subfolder_path)\n",
    "        if not os.path.exists(subfolder_path):\n",
    "            os.makedirs(subfolder_path)\n",
    "        new_name = os.path.join(subfolder_path, video_name.split('_')[2])\n",
    "\n",
    "\n",
    "        \n",
    "        os.makedirs(new_name, exist_ok=True)\n",
    "        extract_frames_from_video(os.path.join(input_folder, video_file), new_name, frame_rate)\n",
    "        #os.rmdir(video_output_folder)  # Delete the video_output_folder after extracting frames\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for item in os.listdir(directory):\n",
    "    item_path = os.path.join(directory, item)\n",
    "    if os.path.isdir(item_path):\n",
    "        convert_videos_to_images(item_path, item_path, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now extract the audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import numpy as np\n",
    "\n",
    "# Load the audio file\n",
    "def audio_cropper(audio_path):\n",
    "    # Read the audio file\n",
    "    with wave.open(audio_path, 'rb') as audio_file:\n",
    "        # Get audio properties\n",
    "        channels = audio_file.getnchannels()\n",
    "        sample_width = audio_file.getsampwidth()\n",
    "        frame_rate = audio_file.getframerate()\n",
    "        frames = audio_file.getnframes()\n",
    "\n",
    "        # Read audio data\n",
    "        audio_data = audio_file.readframes(frames)\n",
    "\n",
    "    # Convert audio data to numpy array\n",
    "    audio = np.frombuffer(audio_data, dtype=np.int16)\n",
    "    # Set the duration to keep\n",
    "    duration = 15  # in seconds\n",
    "\n",
    "    # Calculate the number of frames to keep\n",
    "    frames_to_keep = int(duration * frame_rate)\n",
    "\n",
    "    # Keep only the first 'frames_to_keep' frames\n",
    "    audio = audio[:frames_to_keep]\n",
    "    # Set the threshold for silence detection\n",
    "    threshold = 1000\n",
    "\n",
    "    # Find the indices where the amplitude is higher than the threshold\n",
    "    above_threshold_indices = np.where(audio > threshold)[0]\n",
    "\n",
    "    # Find the last time where the amplitude is higher than the threshold\n",
    "    last_time_above_threshold = above_threshold_indices[-1] / frame_rate\n",
    "\n",
    "    # Remove the audio after the last time_above_threshold\n",
    "    last_index_above_threshold = int(last_time_above_threshold * frame_rate)\n",
    "    audio = audio[:last_index_above_threshold+1*frame_rate]\n",
    "\n",
    "    # Plot the updated amplitude\n",
    "    import matplotlib.pyplot as plt\n",
    "    # Write the updated audio data back to the file\n",
    "    with wave.open(audio_path, 'wb') as audio_file:\n",
    "        audio_file.setnchannels(channels)  # Set the number of channels\n",
    "        audio_file.setframerate(frame_rate)\n",
    "\n",
    "        audio_file.setsampwidth(sample_width)  # Set the sample width\n",
    "        audio_file.setnframes(len(audio))\n",
    "        audio_file.writeframes(audio.tobytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of files in the input directory\n",
    "files = os.listdir(os.path.join(directory,\"audio\"))\n",
    "\n",
    "# Iterate over the files and call the audio_cropper() function\n",
    "for file in files:\n",
    "    file_path = os.path.join(os.path.join(directory,\"audio\"), file)\n",
    "    audio_cropper(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now blur out the face in video 1 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "path_to_image_dir = os.path.join(directory,'video_1')\n",
    "\n",
    "# Iterate over subfolders\n",
    "for subfolder in os.listdir(path_to_image_dir):\n",
    "    subfolder_path = os.path.join(path_to_image_dir, subfolder)\n",
    "    \n",
    "    # Check if the item is a directory\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # Iterate over images in the subfolder\n",
    "        for image_file in os.listdir(subfolder_path):\n",
    "            if \"anonymized\" not in image_file:\n",
    "                continue\n",
    "            #    os.remove(os.path.join(subfolder_path, image_file))\n",
    "            #else:\n",
    "            #    image_path = os.path.join(subfolder_path, image_file)\n",
    "            #    new_image_path = os.path.join(subfolder_path, image_file.replace(\"_anonymized\", \"\"))\n",
    "            #    os.rename(image_path, new_image_path)\n",
    "\n",
    "            image_path = os.path.join(subfolder_path, image_file)\n",
    "            \n",
    "            # Call the deface function for each image\n",
    "            subprocess.call([\"deface\", image_path, image_path,\"--backend\",\"opencv\"])\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now crop out all faces in video 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 2/2 [00:04<00:00,  2.45s/it]\n",
      "Processing: 100%|██████████| 1/1 [00:01<00:00,  1.57s/it]\n",
      "Processing: 100%|██████████| 2/2 [00:01<00:00,  1.22it/s]\n",
      "Processing: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]\n",
      "Processing: 100%|██████████| 2/2 [00:01<00:00,  1.26it/s]\n",
      "Processing: 100%|██████████| 3/3 [00:02<00:00,  1.03it/s]\n",
      "Processing: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s]\n",
      "Processing: 100%|██████████| 3/3 [00:02<00:00,  1.06it/s]\n",
      "Processing: 100%|██████████| 6/6 [00:06<00:00,  1.02s/it]\n",
      "Processing: 100%|██████████| 3/3 [00:03<00:00,  1.07s/it]\n",
      "Processing: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]\n",
      "Processing: 100%|██████████| 2/2 [00:02<00:00,  1.11s/it]\n",
      "Processing: 100%|██████████| 2/2 [00:01<00:00,  1.17it/s]\n",
      "Processing: 100%|██████████| 2/2 [00:01<00:00,  1.03it/s]\n",
      "Processing: 100%|██████████| 3/3 [00:02<00:00,  1.08it/s]\n",
      "Processing: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]\n",
      "Processing: 100%|██████████| 3/3 [00:02<00:00,  1.08it/s]\n",
      "Processing: 100%|██████████| 4/4 [00:03<00:00,  1.02it/s]\n",
      "Processing: 100%|██████████| 5/5 [00:05<00:00,  1.07s/it]\n",
      "Processing: 100%|██████████| 4/4 [00:03<00:00,  1.06it/s]\n",
      "Processing: 100%|██████████| 2/2 [00:02<00:00,  1.20s/it]\n",
      "Processing: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]\n",
      "Processing: 100%|██████████| 2/2 [00:01<00:00,  1.00it/s]\n",
      "Processing: 100%|██████████| 4/4 [00:04<00:00,  1.11s/it]\n",
      "Processing: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]\n",
      "Processing: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]\n",
      "Processing: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]\n",
      "Processing: 100%|██████████| 3/3 [00:03<00:00,  1.05s/it]\n",
      "Processing: 100%|██████████| 2/2 [00:01<00:00,  1.22it/s]\n",
      "Processing: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]\n",
      "Processing: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s]\n",
      "Processing: 100%|██████████| 2/2 [00:02<00:00,  1.20s/it]\n",
      "Processing: 100%|██████████| 3/3 [00:03<00:00,  1.02s/it]\n",
      "Processing: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]\n",
      "Processing: 100%|██████████| 3/3 [00:03<00:00,  1.13s/it]\n",
      "Processing: 100%|██████████| 5/5 [00:05<00:00,  1.18s/it]\n",
      "Processing: 100%|██████████| 2/2 [00:02<00:00,  1.24s/it]\n",
      "Processing: 100%|██████████| 2/2 [00:02<00:00,  1.24s/it]\n",
      "Processing: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]\n",
      "Processing: 100%|██████████| 6/6 [00:07<00:00,  1.18s/it]\n",
      "Processing: 100%|██████████| 4/4 [00:04<00:00,  1.02s/it]\n",
      "Processing: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]\n",
      "Processing: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]\n",
      "Processing: 100%|██████████| 4/4 [00:04<00:00,  1.14s/it]\n",
      "Processing: 100%|██████████| 2/2 [00:02<00:00,  1.27s/it]\n",
      "Processing: 100%|██████████| 6/6 [00:06<00:00,  1.13s/it]\n"
     ]
    }
   ],
   "source": [
    "from face_crop_plus import Cropper\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "cropper = Cropper(face_factor=0.7, strategy=\"largest\",device=\"cuda\")\n",
    "\n",
    "path_to_video_2_dir = os.path.join(directory,'video_2')\n",
    "for task_folder in os.listdir(path_to_video_2_dir):\n",
    "\n",
    "    if os.path.isdir(os.path.join(path_to_video_2_dir,task_folder)):\n",
    "        for episode_folder in os.listdir(os.path.join(path_to_video_2_dir,task_folder)):\n",
    "\n",
    "            if \"face\" in episode_folder:\n",
    "                continue\n",
    "            \n",
    "            folder_path = os.path.join(path_to_video_2_dir, task_folder, episode_folder)\n",
    "            if os.path.isdir(folder_path): \n",
    "                cropper.process_dir(input_dir=folder_path) \n",
    "                shutil.rmtree(folder_path)\n",
    "\n",
    "\n",
    "for task_folder in os.listdir(path_to_video_2_dir):\n",
    "    if os.path.isdir(os.path.join(path_to_video_2_dir, task_folder)):\n",
    "        for episode_folder in os.listdir(os.path.join(path_to_video_2_dir, task_folder)):\n",
    "            if \"face\" in episode_folder:\n",
    "                new_episode_folder = episode_folder.replace(\"_face\", \"\")\n",
    "                folder_path = os.path.join(path_to_video_2_dir, task_folder, episode_folder)\n",
    "                new_folder_path = os.path.join(path_to_video_2_dir, task_folder, new_episode_folder)\n",
    "                os.rename(folder_path, new_folder_path)\n",
    "                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/qihan/Playground/Data/User_3/video_1/1\n",
      "/home/qihan/Playground/Data/User_3/video_1/3\n",
      "/home/qihan/Playground/Data/User_3/video_1/2\n",
      "/home/qihan/Playground/Data/User_3/video_1/1\n",
      "/home/qihan/Playground/Data/User_3/video_1/1\n",
      "/home/qihan/Playground/Data/User_3/video_1/2\n",
      "/home/qihan/Playground/Data/User_3/video_1/0\n",
      "/home/qihan/Playground/Data/User_3/video_1/2\n",
      "/home/qihan/Playground/Data/User_3/video_1/1\n",
      "/home/qihan/Playground/Data/User_3/video_1/0\n",
      "/home/qihan/Playground/Data/User_3/video_1/1\n",
      "/home/qihan/Playground/Data/User_3/video_1/3\n",
      "/home/qihan/Playground/Data/User_3/video_1/3\n",
      "/home/qihan/Playground/Data/User_3/video_1/2\n",
      "/home/qihan/Playground/Data/User_3/video_1/1\n",
      "/home/qihan/Playground/Data/User_3/video_1/2\n",
      "/home/qihan/Playground/Data/User_3/video_1/1\n",
      "/home/qihan/Playground/Data/User_3/video_1/2\n",
      "/home/qihan/Playground/Data/User_3/video_1/2\n",
      "/home/qihan/Playground/Data/User_3/video_1/3\n",
      "/home/qihan/Playground/Data/User_3/video_1/1\n",
      "/home/qihan/Playground/Data/User_3/video_1/1\n",
      "/home/qihan/Playground/Data/User_3/video_1/3\n",
      "/home/qihan/Playground/Data/User_3/video_1/0\n",
      "/home/qihan/Playground/Data/User_3/video_1/0\n",
      "/home/qihan/Playground/Data/User_3/video_1/3\n",
      "/home/qihan/Playground/Data/User_3/video_1/0\n",
      "/home/qihan/Playground/Data/User_3/video_1/1\n",
      "/home/qihan/Playground/Data/User_3/video_1/1\n",
      "/home/qihan/Playground/Data/User_3/video_1/1\n",
      "/home/qihan/Playground/Data/User_3/video_1/1\n",
      "/home/qihan/Playground/Data/User_3/video_1/0\n",
      "/home/qihan/Playground/Data/User_3/video_1/1\n",
      "/home/qihan/Playground/Data/User_3/video_1/3\n",
      "/home/qihan/Playground/Data/User_3/video_1/1\n",
      "/home/qihan/Playground/Data/User_3/video_1/3\n",
      "/home/qihan/Playground/Data/User_3/video_1/2\n",
      "/home/qihan/Playground/Data/User_3/video_1/1\n",
      "/home/qihan/Playground/Data/User_3/video_1/1\n",
      "/home/qihan/Playground/Data/User_3/video_1/2\n",
      "/home/qihan/Playground/Data/User_3/video_1/3\n",
      "/home/qihan/Playground/Data/User_3/video_1/3\n",
      "/home/qihan/Playground/Data/User_3/video_1/1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "audio transcribe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qihan/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"medium.en\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/qihan/Playground/Data/User_3/audio/3_2_0_audio.wav\n",
      " Hi robot, pass me the pink marker.\n",
      "/home/qihan/Playground/Data/User_3/audio/3_2_3_audio.wav\n",
      " Put it back.\n",
      "/home/qihan/Playground/Data/User_3/audio/3_3_0_audio.wav\n",
      " Put the photo frame at top right of the table.\n",
      "/home/qihan/Playground/Data/User_3/audio/3_3_10_audio.wav\n",
      " Hi robot, grab the plant and put it on the left side of the table.\n",
      "/home/qihan/Playground/Data/User_3/audio/3_3_6_audio.wav\n",
      " A robot put the little plant on the left side of the basket on the table.\n",
      "/home/qihan/Playground/Data/User_3/audio/3_2_6_audio.wav\n",
      " Well, give me the pink marker again. Oh, didn't catch it.\n",
      "/home/qihan/Playground/Data/User_3/audio/3_0_0_audio.wav\n",
      " Hi robot, place the cookie in a tray.\n",
      "/home/qihan/Playground/Data/User_3/audio/3_1_15_audio.wav\n",
      " Hi robot, put the ketchup on the left side of the tree.\n",
      "/home/qihan/Playground/Data/User_3/audio/3_2_5_audio.wav\n",
      " Hi robot, give me the red marker.\n",
      "/home/qihan/Playground/Data/User_3/audio/3_2_7_audio.wav\n",
      " Hi, robot. Put the pin- uh, pin marker back.\n",
      "/home/qihan/Playground/Data/User_3/audio/3_1_2_audio.wav\n",
      " Hi robot. Put the pink ice cream between the yellow ice cream and the burger.\n",
      "/home/qihan/Playground/Data/User_3/audio/3_1_11_audio.wav\n",
      " Hi robot, put the sandwich at the top right corner of the tree.\n",
      "/home/qihan/Playground/Data/User_3/audio/3_3_9_audio.wav\n",
      " Hi robot, put the rolls on the left side of the bucket.\n",
      "/home/qihan/Playground/Data/User_3/audio/3_1_0_audio.wav\n",
      " Nevermind. Hi Robob. Put the... Burger at... Uh... Right top corner.\n",
      "/home/qihan/Playground/Data/User_3/audio/3_1_16_audio.wav\n",
      " Put a ketchup in the left part of the tree.\n",
      "/home/qihan/Playground/Data/User_3/audio/3_0_3_audio.wav\n",
      " Put the cookie in the tray.\n",
      "/home/qihan/Playground/Data/User_3/audio/3_2_4_audio.wav\n",
      " Hi robot, put it back.\n",
      "/home/qihan/Playground/Data/User_3/audio/3_1_13_audio.wav\n",
      " We're about to put the burger on the left side of the fries.\n",
      "/home/qihan/Playground/Data/User_3/audio/3_3_8_audio.wav\n",
      " Grab the bucket and put it on the top right side of the table.\n",
      "/home/qihan/Playground/Data/User_3/audio/3_0_4_audio.wav\n",
      " Hi robot, put the cake in the tray.\n",
      "/home/qihan/Playground/Data/User_3/audio/3_1_12_audio.wav\n",
      " Put the fries at bottom right corner of the tree.\n",
      "/home/qihan/Playground/Data/User_3/audio/3_1_17_audio.wav\n",
      " Hi robot, pick up the ketchup, put it in the tray where you have space.\n",
      "/home/qihan/Playground/Data/User_3/audio/3_1_8_audio.wav\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_path\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(file_path)\n\u001b[0;32m----> 7\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Write the result to a text file with the same name as file_path\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/whisper/transcribe.py:240\u001b[0m, in \u001b[0;36mtranscribe\u001b[0;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, **decode_options)\u001b[0m\n\u001b[1;32m    237\u001b[0m mel_segment \u001b[38;5;241m=\u001b[39m pad_or_trim(mel_segment, N_FRAMES)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mto(dtype)\n\u001b[1;32m    239\u001b[0m decode_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m all_tokens[prompt_reset_since:]\n\u001b[0;32m--> 240\u001b[0m result: DecodingResult \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel_segment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(result\u001b[38;5;241m.\u001b[39mtokens)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_speech_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m# no voice activity check\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/whisper/transcribe.py:170\u001b[0m, in \u001b[0;36mtranscribe.<locals>.decode_with_fallback\u001b[0;34m(segment)\u001b[0m\n\u001b[1;32m    167\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_of\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    169\u001b[0m options \u001b[38;5;241m=\u001b[39m DecodingOptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, temperature\u001b[38;5;241m=\u001b[39mt)\n\u001b[0;32m--> 170\u001b[0m decode_result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m needs_fallback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    174\u001b[0m     compression_ratio_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m decode_result\u001b[38;5;241m.\u001b[39mcompression_ratio \u001b[38;5;241m>\u001b[39m compression_ratio_threshold\n\u001b[1;32m    176\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/whisper/decoding.py:824\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(model, mel, options, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[1;32m    822\u001b[0m     options \u001b[38;5;241m=\u001b[39m replace(options, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 824\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mDecodingTask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m single \u001b[38;5;28;01melse\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/whisper/decoding.py:737\u001b[0m, in \u001b[0;36mDecodingTask.run\u001b[0;34m(self, mel)\u001b[0m\n\u001b[1;32m    734\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39mrepeat_interleave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_group, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(audio_features\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    736\u001b[0m \u001b[38;5;66;03m# call the main sampling loop\u001b[39;00m\n\u001b[0;32m--> 737\u001b[0m tokens, sum_logprobs, no_speech_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_main_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;66;03m# reshape the tensors to have (n_audio, n_group) as the first two dimensions\u001b[39;00m\n\u001b[1;32m    740\u001b[0m audio_features \u001b[38;5;241m=\u001b[39m audio_features[:: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_group]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/whisper/decoding.py:687\u001b[0m, in \u001b[0;36mDecodingTask._main_loop\u001b[0;34m(self, audio_features, tokens)\u001b[0m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_len):\n\u001b[0;32m--> 687\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    690\u001b[0m             i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mno_speech \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    691\u001b[0m         ):  \u001b[38;5;66;03m# save no_speech_probs\u001b[39;00m\n\u001b[1;32m    692\u001b[0m             probs_at_sot \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msot_index]\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/whisper/decoding.py:163\u001b[0m, in \u001b[0;36mPyTorchInference.logits\u001b[0;34m(self, tokens, audio_features)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial_token_length:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# only need to use the last token except in the first forward pass\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokens[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/whisper/model.py:211\u001b[0m, in \u001b[0;36mTextDecoder.forward\u001b[0;34m(self, x, xa, kv_cache)\u001b[0m\n\u001b[1;32m    208\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(xa\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 211\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln(x)\n\u001b[1;32m    214\u001b[0m logits \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    215\u001b[0m     x \u001b[38;5;241m@\u001b[39m torch\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype), \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    216\u001b[0m )\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/whisper/model.py:139\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, x, xa, mask, kv_cache)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn:\n\u001b[1;32m    138\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn_ln(x), xa, kv_cache\u001b[38;5;241m=\u001b[39mkv_cache)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 139\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp_ln\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/activation.py:684\u001b[0m, in \u001b[0;36mGELU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 684\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapproximate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapproximate\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "path_to_audio_dir = os.path.join(directory, \"audio\")\n",
    "for file in os.listdir(path_to_audio_dir):\n",
    "    file_path = os.path.join(path_to_audio_dir, file)\n",
    "    if file_path.endswith(\".wav\"):\n",
    "    \n",
    "        print(file_path)\n",
    "        result = model.transcribe(file_path)\n",
    "        print(result[\"text\"])\n",
    "        \n",
    "        # Write the result to a text file with the same name as file_path\n",
    "        output_file_path = file_path.split(\".\")[0] + \".txt\"\n",
    "        with open(output_file_path, 'w') as output_file:\n",
    "            output_file.write(result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def concatenate_text_files(directory, output_file):\n",
    "    lines = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt') and \"all_lines\" not in filename:\n",
    "            user_number = filename.split('_')[0]\n",
    "            task_number = filename.split('_')[1]\n",
    "            index_number = filename.split('_')[2]\n",
    "            with open(os.path.join(directory, filename)) as infile:\n",
    "                for i, line in enumerate(infile, start=1):\n",
    "                    lines.append((int(index_number), line.strip()))\n",
    "    lines.sort(key=lambda x: x[0])  # Sort lines by index number\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        for index, line in lines:\n",
    "            outfile.write(f'{index}: {line};\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            \n",
    "for i in range(0,4):\n",
    "    path_to_audio_dir = os.path.join(directory, \"audio\",str(i))\n",
    "    if not os.path.exists(path_to_audio_dir):\n",
    "        os.mkdir(path_to_audio_dir)\n",
    "  \n",
    "    output_file= os.path.join(path_to_audio_dir,\"all_lines.txt\")\n",
    "    concatenate_text_files(path_to_audio_dir, output_file)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
